{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kailas711/AI-Origins/blob/main/Training%20a%20new%20tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkP76zidk1Ga"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzHQ4CbUk013"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfzjuz_bqMYj"
      },
      "source": [
        "# Training a Tokenizer from an old one\n",
        "\n",
        "**Note**\n",
        "- Training a tokenizer is not the same as training a model!. Model training is very randomized, we use SGD to reduce loss for each batch( each time we get different result hence we use seed to maintain consistecy ).\n",
        "- Training tokenizer involves a statistical method to identify the best subwords for given corpus, and the exact rules used to pick them.\n",
        "It‚Äôs deterministic, meaning you always get the same results when training with the same algorithm on the same corpus.\n",
        "\n",
        "In ü§ó Transformers you can use an API to train a new tokenizer with the same characteristics as an existing one:\n",
        "\n",
        "`AutoTokenizer.train_new_from_iterator()`\n",
        "\n",
        "### 1. Assembling a corpus\n",
        "\n",
        "The CodeSearchNet dataset will be used here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "8a1fea016e354438a7a422d4faf7312b",
            "1bcc6c8945e34a6ba3be613a86a005a1"
          ]
        },
        "id": "MeFGdYw3-7Il",
        "outputId": "d32fb1ee-1839-421a-c2bf-eae2704db159"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a1fea016e354438a7a422d4faf7312b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oqh_n2r2kaVP"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
        "raw_datasets = load_dataset(\"claudios/code_search_net\", \"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjKSebrtsX-8",
        "outputId": "5979e8fd-514f-4843-bc34-d409a4655d0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_documentation_string', 'func_code_url'],\n",
              "    num_rows: 412178\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "raw_datasets[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bwkPZkvUvwy",
        "outputId": "17857a30-f68e-4129-f945-db8bdea59013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def json(self,attribs =None, recurse=True, ignorelist=False):\n",
            "        \"\"\"See :meth:`AbstractElement.json`\"\"\"\n",
            "        if not attribs: attribs = {}\n",
            "        if self.idref:\n",
            "            attribs['id'] = self.idref\n",
            "        return super(AbstractTextMarkup,self).json(attribs,recurse, ignorelist)\n"
          ]
        }
      ],
      "source": [
        "print(raw_datasets[\"train\"][45][\"whole_func_string\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQzBhao1tDVF"
      },
      "source": [
        "Here. we‚Äôll just use the `whole_func_string` column to train our tokenizer.\n",
        "\n",
        "- The first thing we need to do is transform the dataset into an iterator of lists of texts, Using lists of texts will enable our tokenizer to go faster (training on batches of texts instead of processing individual texts one by one).\n",
        "-  iterators avoid having everything in memory at once, we need this!.\n",
        "- ü§ó Datasets does not load everything into RAM but stores the elements of the dataset on disk.\n",
        "\n",
        "\n",
        "Using a Python generator, we can avoid Python loading anything into memory until it‚Äôs actually necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkMIdaoQWcWj",
        "outputId": "eb860b13-5521-4658-f469-a7ece82ae4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object <genexpr> at 0x7f38726e50e0>\n"
          ]
        }
      ],
      "source": [
        "training_corpus = (\n",
        "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        ")\n",
        "print(training_corpus)\n",
        "# print(list(training_corpus))[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKcNKV-wXEkW"
      },
      "source": [
        "- The problem with a generator object is that it can only be used once.\n",
        "- That‚Äôs why we define a function that returns a generator instead\n",
        "- You can also define your generator inside a for loop by using the yield statement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q6q_TjcbWcNE"
      },
      "outputs": [],
      "source": [
        "def get_training_corpus():\n",
        "    dataset = raw_datasets[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"whole_func_string\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training a new tokenizer**\n",
        "\n",
        "Even though we are going to train a new tokenizer, it‚Äôs a good idea to do this to avoid starting entirely from scratch. This way, we won‚Äôt have to specify anything about the tokenization algorithm or the special tokens we want to use; our new tokenizer will be exactly the same as Qwen"
      ],
      "metadata": {
        "id": "ZHRaGpyTj_Sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1N-D_i3Qs-t3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "tokens = old_tokenizer.tokenize(example)\n",
        "tokens"
      ],
      "metadata": {
        "id": "8lRGLuH4n_1a",
        "outputId": "ddc0226f-59c8-44a9-eadc-c74aca60667a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['def',\n",
              " 'ƒ†add',\n",
              " '_',\n",
              " 'n',\n",
              " 'umbers',\n",
              " '(',\n",
              " 'a',\n",
              " ',',\n",
              " 'ƒ†b',\n",
              " '):',\n",
              " 'ƒä',\n",
              " 'ƒ†',\n",
              " 'ƒ†',\n",
              " 'ƒ†',\n",
              " 'ƒ†\"\"\"',\n",
              " 'Add',\n",
              " 'ƒ†the',\n",
              " 'ƒ†two',\n",
              " 'ƒ†numbers',\n",
              " 'ƒ†`',\n",
              " 'a',\n",
              " '`',\n",
              " 'ƒ†and',\n",
              " 'ƒ†`',\n",
              " 'b',\n",
              " '`',\n",
              " '.\"',\n",
              " '\"\"',\n",
              " 'ƒä',\n",
              " 'ƒ†',\n",
              " 'ƒ†',\n",
              " 'ƒ†',\n",
              " 'ƒ†return',\n",
              " 'ƒ†a',\n",
              " 'ƒ†+',\n",
              " 'ƒ†b']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tokenizer has a few special symbols, like ƒ† and ƒä, which denote spaces and newlines, respectively. As we can see, this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels\n",
        "\n",
        "Note that `AutoTokenizer.train_new_from_iterator()` only works if the tokenizer you are using is a ‚Äúfast‚Äù tokenizer."
      ],
      "metadata": {
        "id": "hGRszKBvc8z4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "up3xtN6Qv3Jf"
      },
      "outputs": [],
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"\"\"class LinearLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = torch.randn(input_size, output_size)\n",
        "        self.bias = torch.zeros(output_size)\n",
        "\n",
        "    \"\"\"\n",
        "tokenizer.tokenize(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jxFUIQdcGCP",
        "outputId": "0fac1d65-ba45-4eb0-8a25-950efe8290c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['class',\n",
              " 'ƒ†Linear',\n",
              " 'Layer',\n",
              " '():',\n",
              " 'ƒäƒ†ƒ†ƒ†',\n",
              " 'ƒ†def',\n",
              " 'ƒ†__',\n",
              " 'init',\n",
              " '__(',\n",
              " 'self',\n",
              " ',',\n",
              " 'ƒ†input',\n",
              " '_',\n",
              " 'size',\n",
              " ',',\n",
              " 'ƒ†output',\n",
              " '_',\n",
              " 'size',\n",
              " '):',\n",
              " 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',\n",
              " 'ƒ†self',\n",
              " '.',\n",
              " 'weight',\n",
              " 'ƒ†=',\n",
              " 'ƒ†torch',\n",
              " '.',\n",
              " 'randn',\n",
              " '(',\n",
              " 'input',\n",
              " '_',\n",
              " 'size',\n",
              " ',',\n",
              " 'ƒ†output',\n",
              " '_',\n",
              " 'size',\n",
              " ')',\n",
              " 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†',\n",
              " 'ƒ†self',\n",
              " '.',\n",
              " 'bias',\n",
              " 'ƒ†=',\n",
              " 'ƒ†torch',\n",
              " '.',\n",
              " 'zeros',\n",
              " '(',\n",
              " 'output',\n",
              " '_',\n",
              " 'size',\n",
              " ')',\n",
              " 'ƒäƒäƒ†ƒ†ƒ†ƒ†']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the tokenizer**"
      ],
      "metadata": {
        "id": "ZKETgKBpdX0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH8qWeAFdZo0",
        "outputId": "5a9b85b6-8aba-46fe-a837-d2126cde2aaa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('code-search-net-tokenizer/tokenizer_config.json',\n",
              " 'code-search-net-tokenizer/special_tokens_map.json',\n",
              " 'code-search-net-tokenizer/vocab.json',\n",
              " 'code-search-net-tokenizer/merges.txt',\n",
              " 'code-search-net-tokenizer/added_tokens.json',\n",
              " 'code-search-net-tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Tokenizer from scratch"
      ],
      "metadata": {
        "id": "LU2xWqLBeZQg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEsSEDnjebds"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhAVLEZA41BBoYP01ePPMD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bcc6c8945e34a6ba3be613a86a005a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "8a1fea016e354438a7a422d4faf7312b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_1bcc6c8945e34a6ba3be613a86a005a1"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}