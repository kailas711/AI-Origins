{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c31e9d",
   "metadata": {},
   "source": [
    "# 4. Pretraining LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb4a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import matplotlib \n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14609fd1",
   "metadata": {},
   "source": [
    "### Using GPT to generate text\n",
    "\n",
    "- Initialize the GPT model.\n",
    "- We use dropout of 0.1 or above, but it is reletavely common to train LLM without dropout nowadays. \n",
    "- Modern LLMs don't use bias vectors in the `nn.Linear` layer for the QKV materices, hence we set `qkv_bias: False`.\n",
    "- We reduce the context length (context_length) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens.\n",
    "- Next, we use the generate_text_simple function from the previous chapter to generate text.\n",
    "- In addition, we define two convenience functions, `text_to_token_ids` and `token_ids_to_text`, for converting between token and text representations that we use throughout this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ae8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from supplementary import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b243956",
   "metadata": {},
   "source": [
    "<img src=\"./metadata/09.png\" alt=\" \" style=\"display: block; margin: 0 auto; width:800px; height:auto;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a27b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from supplementary import generate_text_simple\n",
    "\n",
    "def text_to_token(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<endoftext>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimesion\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # Remove batch dim\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb0632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text \n",
      " every effor moves you (% LGBT Telegram Superman communities Observatoryelse Constant berelevision\n"
     ]
    }
   ],
   "source": [
    "start_context = \"every effor moves you\"\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token(tokenizer=tokenizer, text=start_context),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54071d1",
   "metadata": {},
   "source": [
    "- ohhh..that's one way to generate text....\n",
    "- As we can see above, the model does not produce good text because it has not been trained yet\n",
    "- How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?\n",
    "- The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress\n",
    "\n",
    "### 2. Preparing the dataset loaders\n",
    " \n",
    "- We use a relatively small dataset for training the LLM (in fact, only one short story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02580624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap g\n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# First 50 characters\n",
    "print(text_data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a317011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a2879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
