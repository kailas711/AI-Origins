{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daab947f",
   "metadata": {},
   "source": [
    "# Understanding LLM Input Data\n",
    "\n",
    "### 1 Tokenizing text\n",
    "In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters\n",
    "\n",
    "<img src=\"./metadata/02.png\" alt=\"tokenization example\" style=\"display: block; margin: 0 auto; width:700px; height:auto;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c26322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61213cb9",
   "metadata": {},
   "source": [
    "Load the raw text , a public domain dataset to tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d22113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"./datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e462e",
   "metadata": {},
   "source": [
    "- The goal is to tokenize and embedd this text to feed it into an LLM\n",
    "- Let's develop a simple tokenizer based on some sample text that can be then later applied on our text dataset\n",
    "- Let's us regex to remove all the white spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b87e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item]\n",
    "print(preprocessed[:38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b377c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 8405\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens:\", len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7352f",
   "metadata": {},
   "source": [
    "### 2.Converting tokens into token IDs\n",
    "\n",
    "- Next, we convert the text tokens into token IDs that we can process via embedding layers later\n",
    "- For this we need to build up a vocabulary \n",
    "- The vocabulary contains all the unique words in the input text\n",
    "\n",
    "\n",
    "<img src=\"./metadata/03.png\" alt=\"tokenization example\" style=\"display: block; margin: 0 auto; width:700px; height:auto;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7e9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The vocab size is :  1132\n",
      "('loathing', 0)\n",
      "('axioms', 1)\n",
      "('brown', 2)\n",
      "('big', 3)\n",
      "('got', 4)\n",
      "('luxury', 5)\n",
      "('persuasively', 6)\n",
      "('heard', 7)\n",
      "('learned', 8)\n",
      "('man', 9)\n",
      "('much', 10)\n",
      "('reminded', 11)\n",
      "('dragged', 12)\n",
      "('transmute', 13)\n",
      "('diagnosis', 14)\n",
      "('gave', 15)\n",
      "('mighty', 16)\n",
      "('Ah', 17)\n",
      "('deprecatingly', 18)\n",
      "('hardly', 19)\n",
      "('profusion', 20)\n",
      "('Only', 21)\n"
     ]
    }
   ],
   "source": [
    "all_words = set(sorted(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\" The vocab size is : \" , vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "for i,t in enumerate(vocab.items()):\n",
    "    print(t)\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57299456",
   "metadata": {},
   "source": [
    "Let's now put it all together into a tokenizer class\n",
    "- The `encode` function turns text into token IDs\n",
    "- The `decode` function turns token IDs back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1780263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids \n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07084d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded ids  [1068, 906, 650, 947, 518, 1114, 926, 520, 487, 355, 294, 487, 1068, 985, 914, 250, 308, 408, 74, 376, 914]\n",
      "Decoded Text :  \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(\"Encoded ids \" , ids)\n",
    "\n",
    "# Decode back \n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\"Decoded Text : \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfede6c7",
   "metadata": {},
   "source": [
    "### 3. Bytepair encoding\n",
    "\n",
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- In this lecture, we are using the BPE tokenizer from OpenAI's open-source tiktoken library, which implements its core algorithms in Rust to improve computational performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4546dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "tiktoken.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cfe4501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28ec96e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74821b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 8068, 17511, 41, 42, 31271, 52, 56, 4694, 35]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# breaking down unknown words\n",
    "tokenizer.encode(\"SDFGHJKLIUYTSD\", allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38df7a",
   "metadata": {},
   "source": [
    "### 4. Data sampling with a sliding window\n",
    "- Now, let's talk about how we create the data loading for LLMs\n",
    "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict\n",
    "\n",
    "<img src=\"./metadata/04.png\" alt=\"tokenization example\" style=\"display: block; margin: 0 auto; width:700px; height:auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84f65a",
   "metadata": {},
   "source": [
    "**Creating data loader class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee7b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "763d821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.target_ids = []\n",
    "        self.input_ids = []\n",
    "\n",
    "        # Tokenizer the entire dataset\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i+1 : max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a29cc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "with open(\"./datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "max_length = 256\n",
    "stride = 128\n",
    "dataset = GPTDatasetV1(raw_text, tokenizer, max_length, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "835f3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
